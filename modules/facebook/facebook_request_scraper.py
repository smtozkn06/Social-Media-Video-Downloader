import requests
import re
import json
import time
import random
from urllib.parse import urljoin, urlparse, parse_qs
from bs4 import BeautifulSoup
import os

class FacebookRequestScraper:
    def __init__(self, log_callback=None):
        self.log_callback = log_callback
        self.session = requests.Session()
        self.setup_session()
        
    def log(self, message):
        """Log mesajÄ± gÃ¶nderir"""
        if self.log_callback:
            self.log_callback(message)
        else:
            print(message)
    
    def setup_session(self):
        """Request session'Ä±nÄ± kurar"""
        # User-Agent ayarla
        user_agents = [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        ]
        
        headers = {
            'User-Agent': random.choice(user_agents),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'tr-TR,tr;q=0.9,en-US,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        }
        self.session.headers.update(headers)
        
        # Cookie'leri yÃ¼kle
        self.load_cookies()
    
    def load_cookies(self):
        """KaydedilmiÅŸ cookie'leri yÃ¼kler"""
        try:
            cookie_dir = "cookie"
            if not os.path.exists(cookie_dir):
                os.makedirs(cookie_dir)
            
            cookie_file = os.path.join(cookie_dir, "facebook_cookies.json")
            
            if os.path.exists(cookie_file):
                with open(cookie_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                
                for cookie in cookies:
                    self.session.cookies.set(
                        cookie['name'], 
                        cookie['value'], 
                        domain=cookie.get('domain', '.facebook.com')
                    )
                
                self.log(f"âœ… {len(cookies)} adet cookie yÃ¼klendi")
                return True
            else:
                self.log("â„¹ï¸ KaydedilmiÅŸ cookie bulunamadÄ±")
                return False
                
        except Exception as e:
            self.log(f"âŒ Cookie yÃ¼kleme hatasÄ±: {str(e)}")
            return False
    
    def save_cookies(self):
        """Mevcut cookie'leri kaydeder"""
        try:
            cookie_dir = "cookie"
            if not os.path.exists(cookie_dir):
                os.makedirs(cookie_dir)
            
            cookie_file = os.path.join(cookie_dir, "facebook_cookies.json")
            cookies = []
            
            for cookie in self.session.cookies:
                cookies.append({
                    'name': cookie.name,
                    'value': cookie.value,
                    'domain': cookie.domain,
                    'path': cookie.path
                })
            
            with open(cookie_file, 'w', encoding='utf-8') as f:
                json.dump(cookies, f, indent=2)
            
            self.log(f"âœ… {len(cookies)} adet cookie kaydedildi")
            return True
            
        except Exception as e:
            self.log(f"âŒ Cookie kaydetme hatasÄ±: {str(e)}")
            return False
    
    def get_page_content(self, url, retries=3):
        """Sayfa iÃ§eriÄŸini alÄ±r - geliÅŸtirilmiÅŸ hata yÃ¶netimi ile"""
        for attempt in range(retries):
            try:
                # FarklÄ± header kombinasyonlarÄ±nÄ± dene
                header_sets = [
                    {
                        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'DNT': '1',
                        'Connection': 'keep-alive',
                        'Upgrade-Insecure-Requests': '1',
                        'Sec-Fetch-Dest': 'document',
                        'Sec-Fetch-Mode': 'navigate',
                        'Sec-Fetch-Site': 'none',
                        'Cache-Control': 'max-age=0'
                    },
                    {
                        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                        'Accept-Language': 'en-US,en;q=0.5',
                        'Accept-Encoding': 'gzip, deflate, br',
                        'Connection': 'keep-alive'
                    }
                ]
                
                # Ä°lk denemede mevcut session header'larÄ±nÄ± kullan
                if attempt == 0:
                    response = self.session.get(url, timeout=30)
                else:
                    # Sonraki denemelerde farklÄ± header'lar dene
                    headers = header_sets[min(attempt - 1, len(header_sets) - 1)]
                    temp_headers = self.session.headers.copy()
                    temp_headers.update(headers)
                    response = self.session.get(url, headers=temp_headers, timeout=30)
                
                if response.status_code == 200:
                    # Ä°Ã§erik kontrolÃ¼
                    content = response.text
                    if len(content) > 100:  # Minimum iÃ§erik kontrolÃ¼
                        return content
                    else:
                        self.log(f"âš ï¸ Ã‡ok kÄ±sa iÃ§erik alÄ±ndÄ± ({len(content)} karakter), tekrar deneniyor...")
                        continue
                        
                elif response.status_code == 429:
                    # Rate limit
                    wait_time = random.uniform(5, 15)
                    self.log(f"âš ï¸ Rate limit, {wait_time:.1f} saniye bekleniyor...")
                    time.sleep(wait_time)
                    continue
                    
                elif response.status_code == 403:
                    self.log(f"âš ï¸ HTTP 403 hatasÄ± (deneme {attempt + 1}/{retries}), farklÄ± header deneniyor...")
                    if attempt < retries - 1:
                        time.sleep(random.uniform(3, 7))
                    continue
                    
                elif response.status_code == 404:
                    self.log(f"âŒ HTTP 404 hatasÄ±: Sayfa bulunamadÄ± - {url}")
                    return None
                    
                else:
                    self.log(f"âš ï¸ HTTP {response.status_code} hatasÄ± (deneme {attempt + 1}/{retries}): {url}")
                    if attempt < retries - 1:
                        time.sleep(random.uniform(2, 5))
                    continue
                    
            except requests.exceptions.Timeout as e:
                self.log(f"âš ï¸ Timeout hatasÄ± (deneme {attempt + 1}/{retries}): {str(e)}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(3, 8))
                    
            except requests.exceptions.ConnectionError as e:
                self.log(f"âš ï¸ BaÄŸlantÄ± hatasÄ± (deneme {attempt + 1}/{retries}): {str(e)}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(5, 10))
                    
            except requests.exceptions.RequestException as e:
                self.log(f"âš ï¸ Ä°stek hatasÄ± (deneme {attempt + 1}/{retries}): {str(e)}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(2, 5))
                    
            except Exception as e:
                self.log(f"âš ï¸ Beklenmeyen hata (deneme {attempt + 1}/{retries}): {str(e)}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(2, 5))
                    
        self.log(f"âŒ TÃ¼m denemeler baÅŸarÄ±sÄ±z oldu: {url}")
        return None
    
    def get_page_content_with_search_headers(self, url, retries=3):
        """Arama iÃ§in mevcut session header'larÄ± ve cookie'leri kullanarak sayfa iÃ§eriÄŸini alÄ±r"""
        for attempt in range(retries):
            try:
                # Mevcut session header'larÄ±nÄ± kopyala ve arama iÃ§in Ã¶zelleÅŸtir
                search_headers = self.session.headers.copy()
                
                # Arama iÃ§in ek header'lar ekle
                search_headers.update({
                    'Referer': 'https://www.facebook.com/',
                    'Origin': 'https://www.facebook.com',
                    'Sec-Fetch-Site': 'same-origin',
                    'Cache-Control': 'max-age=0'
                })
                
                # Session'daki cookie'ler otomatik olarak kullanÄ±lacak
                response = self.session.get(url, headers=search_headers, timeout=30)
                
                if response.status_code == 200:
                    return response.text
                elif response.status_code == 429:
                    # Rate limit
                    wait_time = random.uniform(5, 15)
                    self.log(f"âš ï¸ Rate limit, {wait_time:.1f} saniye bekleniyor...")
                    time.sleep(wait_time)
                    continue
                else:
                    self.log(f"âš ï¸ HTTP {response.status_code} hatasÄ±: {url}")
                    
            except requests.exceptions.RequestException as e:
                self.log(f"âš ï¸ Ä°stek hatasÄ± (deneme {attempt + 1}/{retries}): {str(e)}")
                if attempt < retries - 1:
                    time.sleep(random.uniform(2, 5))
                    
        return None
    
    def extract_video_urls_from_content(self, content, base_url=""):
        """HTML iÃ§eriÄŸinden video URL'lerini Ã§Ä±karÄ±r"""
        video_urls = set()
        
        try:
            # BeautifulSoup ile parse et
            soup = BeautifulSoup(content, 'html.parser')
            
            # Video link pattern'leri - daha kapsamlÄ±
            video_patterns = [
                r'https?://(?:www\.)?facebook\.com/watch/\?v=\d+',
                r'https?://(?:www\.)?facebook\.com/watch/\?.*v=\d+',
                r'https?://(?:www\.)?facebook\.com/[^/]+/videos/\d+',
                r'https?://(?:www\.)?facebook\.com/video\.php\?v=\d+',
                r'https?://(?:www\.)?facebook\.com/photo\.php\?fbid=\d+',
                r'https?://(?:www\.)?facebook\.com/photo/\?fbid=\d+',
                r'https?://fb\.watch/[A-Za-z0-9_-]+',
                r'https?://(?:www\.)?facebook\.com/reel/\d+',
                r'https?://(?:www\.)?facebook\.com/[^/]+/posts/\d+',
                r'https?://(?:www\.)?facebook\.com/story\.php\?story_fbid=\d+',
                r'https?://(?:www\.)?facebook\.com/permalink\.php\?story_fbid=\d+',
                r'https?://(?:www\.)?facebook\.com/groups/[^/]+/posts/\d+',
                r'https?://(?:www\.)?facebook\.com/groups/[^/]+/permalink/\d+'
            ]
            
            # TÃ¼m linkleri bul
            links = soup.find_all('a', href=True)
            
            for link in links:
                href = link.get('href')
                if href:
                    # Relative URL'leri absolute yap
                    if href.startswith('/'):
                        href = urljoin('https://www.facebook.com', href)
                    
                    # Video pattern'lerini kontrol et
                    for pattern in video_patterns:
                        if re.search(pattern, href):
                            clean_url = self.clean_facebook_url(href)
                            if clean_url:
                                video_urls.add(clean_url)
                                break
            
            # JSON iÃ§indeki URL'leri de ara - daha kapsamlÄ± pattern'ler
            json_patterns = [
                r'"[^"]*url[^"]*"\s*:\s*"([^"]*facebook[^"]*)',
                r'"permalink_url"\s*:\s*"([^"]*facebook[^"]*)',
                r'"link"\s*:\s*"([^"]*facebook[^"]*)',
                r'"href"\s*:\s*"([^"]*facebook[^"]*)',
                r'"story_permalink"\s*:\s*"([^"]*facebook[^"]*)',
                r'https?://(?:www\.)?facebook\.com/[^\s"\'>]*(?:watch|video|photo|reel|posts|story|permalink)[^\s"\'>]*'
            ]
            
            for pattern in json_patterns:
                json_matches = re.findall(pattern, content, re.IGNORECASE)
                
                for match in json_matches:
                    if match:
                        # URL'yi decode et
                        decoded_url = match.replace('\\/', '/').replace('\\u0026', '&').replace('\\u003d', '=')
                        clean_url = self.clean_facebook_url(decoded_url)
                        if clean_url and self.is_valid_facebook_video_url(clean_url):
                            video_urls.add(clean_url)
            
            # Data attribute'larÄ±ndan da ara
            data_elements = soup.find_all(attrs={'data-href': True})
            for element in data_elements:
                data_href = element.get('data-href')
                if data_href and 'facebook.com' in data_href:
                    for pattern in video_patterns:
                        if re.search(pattern, data_href):
                            clean_url = self.clean_facebook_url(data_href)
                            if clean_url:
                                video_urls.add(clean_url)
                                break
            
            return list(video_urls)
            
        except Exception as e:
            self.log(f"âŒ Video URL Ã§Ä±karma hatasÄ±: {str(e)}")
            return []
    
    def clean_facebook_url(self, url):
        """Facebook URL'ini temizler ve standartlaÅŸtÄ±rÄ±r"""
        try:
            if not url or not isinstance(url, str):
                return None
            
            # URL'yi decode et
            url = url.replace('\\/', '/').replace('\\u0026', '&').replace('\\u003d', '=')
            
            # Facebook domain kontrolÃ¼
            if not any(domain in url.lower() for domain in ['facebook.com', 'fb.watch']):
                return None
            
            # Gereksiz parametreleri temizle
            parsed = urlparse(url)
            
            # Query parametrelerini filtrele
            query_params = parse_qs(parsed.query)
            
            # Ã–nemli parametreleri koru
            important_params = {}
            
            if 'v' in query_params:
                important_params['v'] = query_params['v'][0]
            if 'fbid' in query_params:
                important_params['fbid'] = query_params['fbid'][0]
            if 'set' in query_params:
                important_params['set'] = query_params['set'][0]
            if 'story_fbid' in query_params:
                important_params['story_fbid'] = query_params['story_fbid'][0]
            if 'id' in query_params:
                important_params['id'] = query_params['id'][0]
            
            # Temiz URL oluÅŸtur
            clean_query = '&'.join([f"{k}={v}" for k, v in important_params.items()])
            
            clean_url = f"{parsed.scheme}://{parsed.netloc}{parsed.path}"
            if clean_query:
                clean_url += f"?{clean_query}"
            
            return clean_url
            
        except Exception as e:
            self.log(f"âš ï¸ URL temizleme hatasÄ±: {str(e)}")
            return None
    
    def is_valid_facebook_video_url(self, url):
        """Facebook video URL'inin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder"""
        if not url:
            return False
        
        video_patterns = [
            # Video URL'leri
            r'facebook\.com/watch/\?.*v=\d+',
            r'facebook\.com/watch\?v=\d+',
            r'facebook\.com/[^/]+/videos/\d+',
            r'facebook\.com/video\.php\?v=\d+',
            r'fb\.watch/[A-Za-z0-9_-]+',
            
            # Reel URL'leri
            r'facebook\.com/reel/\d+',
            r'facebook\.com/[^/]+/reel/\d+',
            
            # Photo URL'leri - GeniÅŸletilmiÅŸ
            r'facebook\.com/photo\.php\?fbid=\d+',
            r'facebook\.com/photo/\?fbid=\d+',
            r'facebook\.com/photo\?fbid=\d+',
            r'facebook\.com/[^/]+/photos/[^/]+/\d+',
            r'facebook\.com/photo/\?fbid=\d+&set=[^&]+',
            r'facebook\.com/photo\?fbid=\d+&set=[^&]+',
            
            # Post URL'leri
            r'facebook\.com/[^/]+/posts/\d+',
            r'facebook\.com/story\.php\?story_fbid=\d+',
            r'facebook\.com/permalink\.php\?story_fbid=\d+',
            
            # Group URL'leri
            r'facebook\.com/groups/[^/]+/posts/\d+',
            r'facebook\.com/groups/[^/]+/permalink/\d+'
        ]
        
        return any(re.search(pattern, url) for pattern in video_patterns)
    
    def is_valid_facebook_url(self, url):
        """Facebook URL'inin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder (video URL'leri iÃ§in alias)"""
        return self.is_valid_facebook_video_url(url)
    
    def search_videos_from_page(self, page_url, max_videos=50):
        """Belirli bir sayfadan video linklerini arar"""
        self.log(f"ğŸ” Sayfa taranÄ±yor: {page_url}")
        
        content = self.get_page_content(page_url)
        if not content:
            self.log("âŒ Sayfa iÃ§eriÄŸi alÄ±namadÄ±")
            return []
        
        video_urls = self.extract_video_urls_from_content(content, page_url)
        
        # Maksimum video sayÄ±sÄ±nÄ± sÄ±nÄ±rla
        if len(video_urls) > max_videos:
            video_urls = video_urls[:max_videos]
        
        self.log(f"âœ… {len(video_urls)} adet video linki bulundu")
        return video_urls
    
    def search_posts_with_infinite_scroll(self, search_term, max_videos=50):
        """Facebook'ta arama yaparak infinite scroll ile post/video linklerini toplar"""
        self.log(f"ğŸ” Facebook'ta arama yapÄ±lÄ±yor: {search_term}")
        
        # FarklÄ± arama URL formatlarÄ±nÄ± dene
        search_urls = [
            f"https://www.facebook.com/search/top/?q={search_term}",
            f"https://www.facebook.com/search/videos/?q={search_term}",
            f"https://m.facebook.com/search/?q={search_term}",
            f"https://www.facebook.com/public/{search_term}"
        ]
        
        all_video_urls = set()
        
        for search_url in search_urls:
            self.log(f"ğŸ” Denenen arama URL'i: {search_url}")
            
            cursor = None
            page_count = 0
            max_pages = 5  # Her URL iÃ§in maksimum sayfa sayÄ±sÄ±
            
            while len(all_video_urls) < max_videos and page_count < max_pages:
                try:
                    # Ä°lk sayfa veya cursor ile sonraki sayfa
                    if cursor:
                        url = f"{search_url}&cursor={cursor}"
                    else:
                        url = search_url
                    
                    self.log(f"ğŸ“„ Sayfa {page_count + 1} yÃ¼kleniyor...")
                    content = self.get_page_content_with_search_headers(url)
                    
                    if not content:
                        self.log("âŒ Sayfa iÃ§eriÄŸi alÄ±namadÄ±")
                        break
                
                    # Video URL'lerini Ã§Ä±kar
                    page_video_urls = self.extract_video_urls_from_content(content, url)
                    
                    if not page_video_urls:
                        self.log("âš ï¸ Bu sayfada video bulunamadÄ±")
                        # Cursor'u bulmaya Ã§alÄ±ÅŸ
                        cursor = self.extract_next_cursor(content)
                        if not cursor:
                            self.log("âŒ Sonraki sayfa cursor'u bulunamadÄ±")
                            break
                        page_count += 1
                        continue
                    
                    # Yeni URL'leri ekle
                    new_urls = 0
                    for video_url in page_video_urls:
                        if video_url not in all_video_urls:
                            all_video_urls.add(video_url)
                            new_urls += 1
                    
                    self.log(f"âœ… Bu sayfada {new_urls} yeni video bulundu (Toplam: {len(all_video_urls)})")
                    
                    # Sonraki sayfa cursor'unu bul
                    cursor = self.extract_next_cursor(content)
                    if not cursor:
                        self.log("â„¹ï¸ Sonraki sayfa bulunamadÄ±, bu URL iÃ§in arama tamamlandÄ±")
                        break
                    
                    page_count += 1
                    
                    # Rate limiting iÃ§in bekleme
                    time.sleep(random.uniform(2, 4))
                    
                except Exception as e:
                    self.log(f"âŒ Sayfa {page_count + 1} iÅŸlenirken hata: {str(e)}")
                    break
            
            # Bu URL'den yeterli sonuÃ§ alÄ±ndÄ±ysa diÄŸer URL'lere geÃ§me
            if len(all_video_urls) >= max_videos:
                break
                
            # URL'ler arasÄ± bekleme
            time.sleep(random.uniform(1, 3))
        
        # SonuÃ§larÄ± liste olarak dÃ¶ndÃ¼r ve maksimum sayÄ±yÄ± sÄ±nÄ±rla
        result_urls = list(all_video_urls)[:max_videos]
        self.log(f"ğŸ‰ Toplam {len(result_urls)} video/post linki bulundu")
        
        return result_urls
    
    def extract_next_cursor(self, content):
        """Sayfa iÃ§eriÄŸinden sonraki sayfa iÃ§in cursor deÄŸerini Ã§Ä±karÄ±r"""
        try:
            # Facebook'un infinite scroll iÃ§in kullandÄ±ÄŸÄ± cursor pattern'leri
            cursor_patterns = [
                r'"cursor"\s*:\s*"([^"]+)"',
                r'"end_cursor"\s*:\s*"([^"]+)"',
                r'"page_info"[^}]*"end_cursor"\s*:\s*"([^"]+)"',
                r'"after"\s*:\s*"([^"]+)"',
                r'data-cursor="([^"]+)"',
                r'cursor=([^&\s"]+)',
                r'"next"[^}]*"cursor"\s*:\s*"([^"]+)"'
            ]
            
            for pattern in cursor_patterns:
                matches = re.findall(pattern, content, re.IGNORECASE)
                if matches:
                    cursor = matches[-1]  # Son cursor'u al
                    if cursor and cursor != "null" and len(cursor) > 5:
                        return cursor
            
            return None
            
        except Exception as e:
            self.log(f"âš ï¸ Cursor Ã§Ä±karma hatasÄ±: {str(e)}")
            return None
    
    def search_videos_from_profile(self, profile_url, max_videos=50):
        """Profil sayfasÄ±ndan video linklerini arar"""
        self.log(f"ğŸ‘¤ Profil taranÄ±yor: {profile_url}")
        
        # Profil sayfasÄ±nÄ±n video sekmesine git
        videos_url = f"{profile_url.rstrip('/')}/videos"
        
        return self.search_videos_from_page(videos_url, max_videos)
    
    def download_video_info(self, video_url):
        """Video bilgilerini indirir"""
        try:
            self.log(f"ğŸ“¹ Video bilgileri alÄ±nÄ±yor: {video_url}")
            
            content = self.get_page_content(video_url)
            if not content:
                return None
            
            # Video baÅŸlÄ±ÄŸÄ±nÄ± bul
            title_patterns = [
                r'<title[^>]*>([^<]+)</title>',
                r'"title"\s*:\s*"([^"]+)"',
                r'"name"\s*:\s*"([^"]+)"'
            ]
            
            title = "Facebook Video"
            for pattern in title_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    title = match.group(1).strip()
                    break
            
            # Video aÃ§Ä±klamasÄ±nÄ± bul
            description_patterns = [
                r'"description"\s*:\s*"([^"]+)"',
                r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\'>]+)["\']',
                r'<meta[^>]*property=["\']og:description["\'][^>]*content=["\']([^"\'>]+)["\']'
            ]
            
            description = ""
            for pattern in description_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    description = match.group(1).strip()
                    break
            
            return {
                'url': video_url,
                'title': title,
                'description': description,
                'timestamp': time.time()
            }
            
        except Exception as e:
            self.log(f"âŒ Video bilgi alma hatasÄ±: {str(e)}")
            return None
    
    def extract_photo_url(self, photo_page_url):
        """Facebook fotoÄŸraf sayfasÄ±ndan gerÃ§ek fotoÄŸraf URL'ini Ã§Ä±karÄ±r - geliÅŸtirilmiÅŸ"""
        try:
            self.log(f"ğŸ“¸ FotoÄŸraf URL'i Ã§Ä±karÄ±lÄ±yor: {photo_page_url}")
            
            # Profil fotoÄŸrafÄ± kontrolÃ¼ - baÅŸlangÄ±Ã§ta kontrol et
            if self._is_profile_photo(photo_page_url):
                self.log(f"â­ï¸ Profil/kapak fotoÄŸrafÄ± URL'i atlanÄ±yor: {photo_page_url}")
                return None
            
            # EÄŸer URL zaten direkt fotoÄŸraf URL'i ise
            if any(domain in photo_page_url for domain in ['scontent', 'fbcdn']) and any(ext in photo_page_url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
                self.log(f"âœ… Direkt fotoÄŸraf URL'i tespit edildi")
                cleaned_url = self._clean_photo_url(photo_page_url)
                # Direkt URL iÃ§in de profil kontrolÃ¼ yap
                if cleaned_url and self._is_profile_photo(cleaned_url):
                    self.log(f"â­ï¸ Direkt profil/kapak fotoÄŸrafÄ± URL'i atlanÄ±yor: {cleaned_url}")
                    return None
                return cleaned_url
            
            # fbid parametresini Ã§Ä±kar
            fbid_match = re.search(r'fbid=([0-9]+)', photo_page_url)
            if fbid_match:
                fbid = fbid_match.group(1)
                self.log(f"ğŸ“‹ FBID bulundu: {fbid}")
                
                # FarklÄ± URL formatlarÄ±nÄ± dene
                url_formats = [
                    f"https://www.facebook.com/photo.php?fbid={fbid}",
                    f"https://m.facebook.com/photo.php?fbid={fbid}",
                    f"https://www.facebook.com/photo/?fbid={fbid}",
                    f"https://m.facebook.com/photo/?fbid={fbid}",
                    photo_page_url,
                    photo_page_url.replace('www.facebook.com', 'm.facebook.com')
                ]
            else:
                # fbid bulunamazsa orijinal URL'leri kullan
                url_formats = [
                    photo_page_url,
                    photo_page_url.replace('www.facebook.com', 'm.facebook.com'),
                    photo_page_url.replace('m.facebook.com', 'www.facebook.com')
                ]
            
            for url in url_formats:
                self.log(f"ğŸ” Denenen URL: {url}")
                content = self.get_page_content(url)
                if not content:
                    continue
                
                photo_urls = set()
                
                # BeautifulSoup ile img tag'lerini bul
                soup = BeautifulSoup(content, 'html.parser')
                
                # FarklÄ± img selector'larÄ± dene
                img_selectors = [
                    'img[src*="scontent"]',
                    'img[src*="fbcdn"]',
                    'img[data-src*="scontent"]',
                    'img[data-src*="fbcdn"]',
                    'img[data-original*="scontent"]',
                    'img[data-original*="fbcdn"]',
                    'img'
                ]
                
                for selector in img_selectors:
                    imgs = soup.select(selector)
                    for img in imgs:
                        for attr in ['src', 'data-src', 'data-original', 'data-lazy-src']:
                            src = img.get(attr)
                            if src and self._is_valid_photo_url(src) and not self._is_profile_photo(src):
                                photo_urls.add(self._clean_photo_url(src))
                
                # JSON iÃ§indeki URL'leri ara - daha kapsamlÄ± pattern'ler (boyut parametreleri dahil)
                json_patterns = [
                    r'"url"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    r'"src"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    r'"image"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    r'"full_picture"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    r'"picture"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    r'"photo_image"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    r'"hd_src"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    r'"image_src"\s*:\s*"([^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*)",?',
                    # Boyut parametreli URL'ler iÃ§in Ã¶zel pattern'ler
                    r'"(https?://[^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*[?&]w=[0-9]+[^"]*)',
                    r'"(https?://[^"]*(?:scontent|fbcdn)[^"]*\.(?:jpg|jpeg|png|webp)[^"]*[?&]h=[0-9]+[^"]*)',
                    r'https://[^\s"\'>]*(?:scontent|fbcdn)[^\s"\'>]*\.(?:jpg|jpeg|png|webp)[^\s"\'>]*'
                ]
                
                for pattern in json_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        if isinstance(match, tuple):
                            match = match[0]
                        
                        clean_url = self._clean_photo_url(match)
                        if clean_url and self._is_valid_photo_url(clean_url) and not self._is_profile_photo(clean_url):
                            photo_urls.add(clean_url)
                
                # Meta tag'lerden de ara
                meta_patterns = [
                    r'<meta[^>]*property=["\']og:image["\'][^>]*content=["\']([^"\'>]*(?:scontent|fbcdn)[^"\'>]*\.(?:jpg|jpeg|png|webp)[^"\'>]*)["\']',
                    r'<meta[^>]*name=["\']twitter:image["\'][^>]*content=["\']([^"\'>]*(?:scontent|fbcdn)[^"\'>]*\.(?:jpg|jpeg|png|webp)[^"\'>]*)["\']'
                ]
                
                for pattern in meta_patterns:
                    matches = re.findall(pattern, content, re.IGNORECASE)
                    for match in matches:
                        clean_url = self._clean_photo_url(match)
                        if clean_url and self._is_valid_photo_url(clean_url) and not self._is_profile_photo(clean_url):
                            photo_urls.add(clean_url)
                
                if photo_urls:
                    # En yÃ¼ksek kaliteli URL'yi seÃ§
                    best_url = self._select_best_photo_url(photo_urls)
                    self.log(f"âœ… FotoÄŸraf URL'i bulundu: {best_url[:100]}...")
                    return best_url
            
            self.log("âŒ FotoÄŸraf URL'i bulunamadÄ±")
            return None
                
        except Exception as e:
            self.log(f"âŒ FotoÄŸraf URL Ã§Ä±karma hatasÄ±: {str(e)}")
            return None
    
    def _is_valid_photo_url(self, url):
        """FotoÄŸraf URL'inin geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol eder (profil resimlerini filtreler)"""
        if not url or not isinstance(url, str):
            return False
        
        # Facebook CDN kontrolÃ¼
        if not any(domain in url for domain in ['scontent', 'fbcdn']):
            return False
        
        # Dosya uzantÄ±sÄ± kontrolÃ¼
        if not any(ext in url.lower() for ext in ['.jpg', '.jpeg', '.png', '.webp']):
            return False
        
        # Profil resimlerini filtrele
        profile_indicators = [
            '/p50x50/',     # KÃ¼Ã§Ã¼k profil resmi
            '/p100x100/',   # Orta profil resmi
            '/p200x200/',   # BÃ¼yÃ¼k profil resmi
            '/s50x50/',     # Square profil resmi
            '/s100x100/',   # Square profil resmi
            '/s200x200/',   # Square profil resmi
            '/c50.',        # Crop profil resmi
            '/c100.',       # Crop profil resmi
            '/c200.',       # Crop profil resmi
            'profile_pic',  # Profil resmi gÃ¶stergesi
            '/safe_image',  # Safe image (genellikle profil)
            't39.30808-1',  # Facebook profil resmi type
            'cp0/e15/q65', # Profil resmi parametreleri
        ]
        
        # URL'de profil resmi gÃ¶stergesi varsa reddet
        for indicator in profile_indicators:
            if indicator in url.lower():
                return False
        
        # Ã‡ok kÃ¼Ã§Ã¼k boyutlu resimleri filtrele (genellikle profil resimleri)
        size_patterns = [
            r'[&?]w=([0-9]+)',
            r'[&?]h=([0-9]+)',
            r'/([0-9]+)x([0-9]+)/',
        ]
        
        for pattern in size_patterns:
            matches = re.findall(pattern, url)
            for match in matches:
                if isinstance(match, tuple):
                    # Tuple durumunda (width, height)
                    width, height = int(match[0]), int(match[1])
                    if width < 300 or height < 300:  # 300px'den kÃ¼Ã§Ã¼k resimleri filtrele
                        return False
                else:
                    # Tek deÄŸer durumunda
                    size = int(match)
                    if size < 300:  # 300px'den kÃ¼Ã§Ã¼k resimleri filtrele
                        return False
        
        return True
    
    def _clean_photo_url(self, url):
        """FotoÄŸraf URL'ini temizler"""
        if not url:
            return None
        
        # Escape karakterlerini temizle
        url = url.replace('\\/', '/').replace('\\u0026', '&').replace('&amp;', '&')
        
        # URL decode
        try:
            from urllib.parse import unquote
            url = unquote(url)
        except:
            pass
        
        return url
    
    def _select_best_photo_url(self, photo_urls):
        """En iyi kaliteli fotoÄŸraf URL'ini seÃ§er - hedef boyut: 1290x1720 px (3:4 aspect ratio)"""
        if not photo_urls:
            return None
        
        # Hedef boyutlar
        target_width = 1290
        target_height = 1720
        target_aspect_ratio = 3/4  # 0.75
        
        # URL'leri kalite kriterlerine gÃ¶re sÄ±rala
        def quality_score(url):
            score = 0
            
            # Boyut parametrelerini URL'den Ã§Ä±karmaya Ã§alÄ±ÅŸ
            width_match = re.search(r'[&?]w=([0-9]+)', url)
            height_match = re.search(r'[&?]h=([0-9]+)', url)
            
            if width_match and height_match:
                width = int(width_match.group(1))
                height = int(height_match.group(1))
                aspect_ratio = width / height
                
                # Hedef boyutlara yakÄ±nlÄ±k puanÄ±
                width_diff = abs(width - target_width) / target_width
                height_diff = abs(height - target_height) / target_height
                aspect_diff = abs(aspect_ratio - target_aspect_ratio) / target_aspect_ratio
                
                # Boyut yakÄ±nlÄ±ÄŸÄ± puanÄ± (0-100)
                size_score = max(0, 100 - (width_diff + height_diff) * 50)
                aspect_score = max(0, 50 - aspect_diff * 50)
                
                score += size_score + aspect_score
                
                # Hedef boyuttan bÃ¼yÃ¼k gÃ¶rÃ¼ntÃ¼leri tercih et
                if width >= target_width and height >= target_height:
                    score += 50
                
                self.log(f"ğŸ“ URL boyut analizi: {width}x{height} (AR: {aspect_ratio:.2f}) - Puan: {score:.1f}")
            else:
                # Boyut bilgisi yoksa, geleneksel kalite gÃ¶stergelerini kullan
                if '_o.' in url or '_n.' in url:  # Orijinal veya bÃ¼yÃ¼k boyut
                    score += 100
                elif '_b.' in url:  # BÃ¼yÃ¼k boyut
                    score += 80
                elif '_c.' in url:  # Orta boyut
                    score += 60
                elif '_s.' in url:  # KÃ¼Ã§Ã¼k boyut
                    score += 40
                
                # URL uzunluÄŸu (genellikle daha uzun = daha fazla parametre = daha iyi kalite)
                score += len(url) / 10
            
            return score
        
        best_url = max(photo_urls, key=quality_score)
        self.log(f"ğŸ¯ En uygun fotoÄŸraf URL'i seÃ§ildi: {best_url[:100]}...")
        return best_url
    
    def download_photo(self, photo_url, output_path):
        """FotoÄŸrafÄ± indirir - geliÅŸtirilmiÅŸ hata yÃ¶netimi ile"""
        try:
            # Profil fotoÄŸrafÄ± kontrolÃ¼ - profil fotoÄŸraflarÄ±nÄ± indirme
            if self._is_profile_photo(photo_url):
                self.log(f"â­ï¸ Profil/kapak fotoÄŸrafÄ± atlanÄ±yor: {photo_url}")
                return False
            
            self.log(f"ğŸ“¥ FotoÄŸraf indiriliyor: {photo_url}")
            
            # FotoÄŸraf URL'ini Ã§Ä±kar
            if 'photo' in photo_url and not any(domain in photo_url for domain in ['scontent', 'fbcdn']):
                actual_photo_url = self.extract_photo_url(photo_url)
                if not actual_photo_url:
                    self.log(f"âŒ FotoÄŸraf URL'i Ã§Ä±karÄ±lamadÄ±: {photo_url}")
                    return False
            else:
                actual_photo_url = photo_url
                
            # URL geÃ§erliliÄŸini kontrol et
            if not self._is_valid_photo_url(actual_photo_url):
                self.log(f"âŒ GeÃ§ersiz fotoÄŸraf URL'i: {actual_photo_url}")
                return False
            
            # FarklÄ± header kombinasyonlarÄ±nÄ± dene
            header_sets = [
                {
                    'Referer': 'https://www.facebook.com/',
                    'Accept': 'image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',
                    'Sec-Fetch-Dest': 'image',
                    'Sec-Fetch-Mode': 'no-cors',
                    'Sec-Fetch-Site': 'cross-site',
                    'Cache-Control': 'no-cache'
                },
                {
                    'Referer': photo_url if 'facebook.com' in photo_url else 'https://m.facebook.com/',
                    'Accept': 'image/*,*/*;q=0.8',
                    'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1'
                },
                {
                    'Accept': '*/*',
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
                }
            ]
            
            for i, headers in enumerate(header_sets):
                try:
                    # Mevcut session header'larÄ±nÄ± koru ve yeni header'larÄ± ekle
                    temp_headers = self.session.headers.copy()
                    temp_headers.update(headers)
                    
                    response = self.session.get(
                        actual_photo_url, 
                        headers=temp_headers, 
                        timeout=30,
                        stream=True
                    )
                    
                    if response.status_code == 200:
                        # Dosya uzantÄ±sÄ±nÄ± belirle
                        content_type = response.headers.get('content-type', '')
                        if 'jpeg' in content_type or 'jpg' in content_type:
                            ext = '.jpg'
                        elif 'png' in content_type:
                            ext = '.png'
                        elif 'webp' in content_type:
                            ext = '.webp'
                        else:
                            # URL'den uzantÄ±yÄ± Ã§Ä±karmaya Ã§alÄ±ÅŸ
                            url_ext = re.search(r'\.(jpg|jpeg|png|webp)', actual_photo_url, re.IGNORECASE)
                            ext = url_ext.group(0) if url_ext else '.jpg'
                        
                        # Dosya adÄ±nÄ± oluÅŸtur
                        if not output_path.endswith(('.jpg', '.jpeg', '.png', '.webp')):
                            output_path += ext
                        
                        # DosyayÄ± kaydet
                        os.makedirs(os.path.dirname(output_path), exist_ok=True)
                        
                        with open(output_path, 'wb') as f:
                            for chunk in response.iter_content(chunk_size=8192):
                                if chunk:
                                    f.write(chunk)
                        
                        file_size = os.path.getsize(output_path)
                        
                        # Dosya boyutu kontrolÃ¼ (Ã§ok kÃ¼Ã§Ã¼kse hata olabilir)
                        if file_size < 1024:  # 1KB'den kÃ¼Ã§Ã¼kse
                            self.log(f"âš ï¸ Dosya Ã§ok kÃ¼Ã§Ã¼k ({file_size} bytes), baÅŸka yÃ¶ntem deneniyor...")
                            os.remove(output_path)
                            continue
                        
                        self.log(f"âœ… FotoÄŸraf indirildi: {output_path} ({file_size} bytes)")
                        return True
                        
                    elif response.status_code == 403:
                        self.log(f"âš ï¸ HTTP 403 hatasÄ± (deneme {i+1}/{len(header_sets)}), farklÄ± header deneniyor...")
                        continue
                    else:
                        self.log(f"âš ï¸ HTTP {response.status_code} hatasÄ± (deneme {i+1}/{len(header_sets)})")
                        continue
                        
                except requests.exceptions.RequestException as e:
                    self.log(f"âš ï¸ Ä°stek hatasÄ± (deneme {i+1}/{len(header_sets)}): {str(e)}")
                    continue
            
            self.log("âŒ TÃ¼m indirme yÃ¶ntemleri baÅŸarÄ±sÄ±z oldu")
            return False
                
        except Exception as e:
            self.log(f"âŒ FotoÄŸraf indirme hatasÄ±: {str(e)}")
            return False
    
    def _is_profile_photo(self, url):
        """URL'nin profil fotoÄŸrafÄ± olup olmadÄ±ÄŸÄ±nÄ± kontrol eder - GeliÅŸtirilmiÅŸ versiyon"""
        if not url:
            return False
        
        # Profil fotoÄŸrafÄ± gÃ¶stergeleri - GeniÅŸletilmiÅŸ liste
        profile_indicators = [
            'profile',
            'profilepic', 
            'profile_pic',
            'profile_picture',
            'avatar',
            'user_photo',
            'profile_image',
            'profile_img',
            '/pp/',
            'type=1',  # Facebook profil fotoÄŸrafÄ± type parametresi
            'type=3',  # Facebook kapak fotoÄŸrafÄ± type parametresi
            'profile.php',
            'picture.php',
            'picture?',
            'picture/',
            'profilepicture',
            'user_avatar',
            'user_pic',
            'user_image',
            'cover_photo',
            'cover_pic',
            'cover_image',
            'coverphoto',
            'timeline_cover',
            'banner_image',
            'header_image',
            'dp.',  # Display picture kÄ±saltmasÄ±
            'pfp.',  # Profile picture kÄ±saltmasÄ±
            '/profile/',
            '/avatar/',
            '/user/',
            'profile_photos',
            'timeline_photos',
            'mobile_photos',
            'profile_timeline',
            'set=a.profile',
            'set=a.timeline',
            'set=profile',
            'set=timeline',
            'set=cover',
            'album_id=profile',
            'album_id=timeline',
            'album_type=profile',
            'album_type=timeline'
        ]
        
        url_lower = url.lower()
        
        # Temel gÃ¶stergeleri kontrol et
        for indicator in profile_indicators:
            if indicator in url_lower:
                self.log(f"â­ï¸ Profil/kapak fotoÄŸrafÄ± tespit edildi ('{indicator}'): {url[:100]}...")
                return True
        
        # URL pattern'leri ile daha detaylÄ± kontrol
        profile_patterns = [
            r'/photos/[^/]+/profile',
            r'/photos/[^/]+/timeline',
            r'/photos/[^/]+/cover',
            r'/photo\.php\?fbid=\d+.*type=[13]',
            r'/photo/\?fbid=\d+.*type=[13]',
            r'/photos/profile/',
            r'/photos/timeline/',
            r'/photos/cover/',
            r'album_id=\d+.*type=[13]',
            r'set=a\.\d+.*type=[13]',
            r'profile.*picture',
            r'timeline.*picture',
            r'cover.*picture'
        ]
        
        for pattern in profile_patterns:
            if re.search(pattern, url_lower):
                self.log(f"â­ï¸ Profil/kapak fotoÄŸrafÄ± pattern tespit edildi: {url[:100]}...")
                return True
        
        # Boyut kontrolÃ¼ - Profil fotoÄŸraflarÄ± genellikle kare veya belirli boyutlarda olur
        size_patterns = [
            r'[&?]w=([0-9]+)',
            r'[&?]h=([0-9]+)',
            r'[&?]width=([0-9]+)',
            r'[&?]height=([0-9]+)'
        ]
        
        width = height = None
        for pattern in size_patterns:
            match = re.search(pattern, url_lower)
            if match:
                size = int(match.group(1))
                if 'w=' in pattern or 'width=' in pattern:
                    width = size
                elif 'h=' in pattern or 'height=' in pattern:
                    height = size
        
        # Profil fotoÄŸrafÄ± boyut kontrolÃ¼ (genellikle 160x160, 320x320, 480x480 gibi)
        if width and height:
            # Kare fotoÄŸraflar ve kÃ¼Ã§Ã¼k boyutlar profil fotoÄŸrafÄ± olabilir
            if width == height and width <= 500:
                self.log(f"â­ï¸ Profil fotoÄŸrafÄ± boyut tespit edildi ({width}x{height}): {url[:100]}...")
                return True
            
            # YaygÄ±n profil fotoÄŸrafÄ± boyutlarÄ±
            common_profile_sizes = [(160, 160), (320, 320), (480, 480), (200, 200), (100, 100), (50, 50)]
            if (width, height) in common_profile_sizes:
                self.log(f"â­ï¸ Standart profil fotoÄŸrafÄ± boyutu tespit edildi ({width}x{height}): {url[:100]}...")
                return True
        
        return False
    
    def download_video_with_http_api(self, video_url, output_path):
        """HTTP/API kullanarak video indirir"""
        try:
            self.log(f"ğŸŒ HTTP/API ile video indiriliyor: {video_url}")
            
            # Facebook Graph API kullanarak video bilgilerini al
            video_info = self._get_video_info_from_api(video_url)
            if video_info and 'video_url' in video_info:
                self.log(f"âœ… API'den video URL alÄ±ndÄ±: {video_info['video_url'][:100]}...")
                return self._download_video_file(video_info['video_url'], output_path)
            
            # API baÅŸarÄ±sÄ±z olursa, HTTP scraping ile dene
            return self._download_video_with_http_scraping(video_url, output_path)
            
        except Exception as e:
            self.log(f"âŒ HTTP/API video indirme hatasÄ±: {str(e)}")
            return False
    
    def _get_video_info_from_api(self, video_url):
        """Facebook Graph API kullanarak video bilgilerini alÄ±r"""
        try:
            # Video ID'sini URL'den Ã§Ä±kar
            video_id = self._extract_video_id(video_url)
            if not video_id:
                return None
            
            # Graph API endpoint'i
            api_url = f"https://graph.facebook.com/v18.0/{video_id}"
            
            # API parametreleri
            params = {
                'fields': 'source,title,description,length,created_time,picture',
                'access_token': self._get_public_access_token()
            }
            
            response = self.session.get(api_url, params=params, timeout=30)
            
            if response.status_code == 200:
                data = response.json()
                if 'source' in data:
                    return {
                        'video_url': data['source'],
                        'title': data.get('title', ''),
                        'description': data.get('description', ''),
                        'duration': data.get('length', 0),
                        'thumbnail': data.get('picture', '')
                    }
            
            return None
            
        except Exception as e:
            self.log(f"âš ï¸ API video bilgisi alÄ±namadÄ±: {str(e)}")
            return None
    
    def _extract_video_id(self, video_url):
        """Video URL'sinden video ID'sini Ã§Ä±karÄ±r"""
        try:
            # FarklÄ± Facebook video URL formatlarÄ± iÃ§in regex pattern'leri
            patterns = [
                r'/videos/(?:vb\.\d+/)?([0-9]+)',
                r'/watch/\?v=([0-9]+)',
                r'/reel/([0-9]+)',
                r'video_id=([0-9]+)',
                r'/([0-9]{10,})/?',
                r'fbid=([0-9]+)'
            ]
            
            for pattern in patterns:
                match = re.search(pattern, video_url)
                if match:
                    return match.group(1)
            
            return None
            
        except Exception:
            return None
    
    def _get_public_access_token(self):
        """Genel eriÅŸim token'Ä± alÄ±r"""
        # Facebook'un genel API eriÅŸimi iÃ§in app token formatÄ±
        # Bu gerÃ§ek bir production uygulamasÄ±nda gÃ¼venli bir ÅŸekilde saklanmalÄ±
        return "your_app_id|your_app_secret"  # Placeholder
    
    def _download_video_with_http_scraping(self, video_url, output_path):
        """HTTP scraping ile video indirir"""
        try:
            self.log("ğŸ” HTTP scraping ile video URL'i aranÄ±yor...")
            
            # Sayfa iÃ§eriÄŸini al
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Referer': 'https://www.facebook.com/',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = self.session.get(video_url, headers=headers, timeout=30)
            
            if response.status_code != 200:
                self.log(f"âŒ Sayfa yÃ¼klenemedi: HTTP {response.status_code}")
                return False
            
            content = response.text
            
            # Video URL pattern'leri (Facebook'un farklÄ± formatlarÄ± iÃ§in)
            video_patterns = [
                r'"playable_url":"([^"]+)"',
                r'"playable_url_quality_hd":"([^"]+)"',
                r'"video_url":"([^"]+)"',
                r'"src":"([^"]+\.mp4[^"]*?)"',
                r'"url":"([^"]+\.mp4[^"]*?)"',
                r'hd_src:"([^"]+)"',
                r'sd_src:"([^"]+)"',
                r'"representation_url":"([^"]+)"',
                r'"browser_native_hd_url":"([^"]+)"',
                r'"browser_native_sd_url":"([^"]+)"'
            ]
            
            video_urls = []
            
            for pattern in video_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    # URL'yi decode et
                    decoded_url = match.replace('\\/', '/').replace('\\u0026', '&').replace('\\u003d', '=')
                    decoded_url = decoded_url.replace('%3A', ':').replace('%2F', '/').replace('%3F', '?').replace('%3D', '=')
                    
                    if decoded_url.startswith('http') and any(ext in decoded_url.lower() for ext in ['.mp4', '.webm', '.avi']):
                        video_urls.append(decoded_url)
            
            if not video_urls:
                self.log("âŒ Sayfa iÃ§eriÄŸinde video URL'i bulunamadÄ±")
                return False
            
            # En iyi kaliteli URL'i seÃ§ (genellikle HD olan)
            best_url = self._select_best_video_url(video_urls)
            self.log(f"âœ… Video URL bulundu: {best_url[:100]}...")
            
            # Video dosyasÄ±nÄ± indir
            return self._download_video_file(best_url, output_path)
            
        except Exception as e:
            self.log(f"âŒ HTTP scraping video indirme hatasÄ±: {str(e)}")
            return False
    
    def _select_best_video_url(self, video_urls):
        """En iyi kaliteli video URL'ini seÃ§er"""
        if not video_urls:
            return None
        
        # HD kalite gÃ¶stergeleri
        hd_indicators = ['hd', 'high', '720', '1080', 'quality_hd']
        
        # HD URL'leri Ã¶ncelikle seÃ§
        hd_urls = []
        for url in video_urls:
            url_lower = url.lower()
            if any(indicator in url_lower for indicator in hd_indicators):
                hd_urls.append(url)
        
        if hd_urls:
            # En uzun HD URL'i seÃ§ (genellikle daha fazla parametre = daha iyi kalite)
            return max(hd_urls, key=len)
        
        # HD bulunamazsa, en uzun URL'i seÃ§
        return max(video_urls, key=len)
    
    def _download_video_file(self, video_url, output_path):
        """Video dosyasÄ±nÄ± indirir"""
        try:
            self.log(f"ğŸ“¥ Video dosyasÄ± indiriliyor: {video_url}")
            
            # Video header'larÄ±
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'video/webm,video/ogg,video/*;q=0.9,application/ogg;q=0.7,audio/*;q=0.6,*/*;q=0.5',
                'Accept-Encoding': 'identity;q=1, *;q=0',
                'Accept-Language': 'en-US,en;q=0.5',
                'Range': 'bytes=0-',
                'Referer': 'https://www.facebook.com/'
            }
            
            response = self.session.get(video_url, headers=headers, stream=True, timeout=60)
            
            if response.status_code in [200, 206]:  # 206 for partial content
                # Dosya uzantÄ±sÄ±nÄ± belirle
                content_type = response.headers.get('content-type', '')
                if 'mp4' in content_type:
                    ext = '.mp4'
                elif 'webm' in content_type:
                    ext = '.webm'
                elif 'avi' in content_type:
                    ext = '.avi'
                else:
                    ext = '.mp4'  # VarsayÄ±lan
                
                # Dosya adÄ±nÄ± oluÅŸtur
                if not output_path.endswith(('.mp4', '.webm', '.avi', '.mov')):
                    output_path += ext
                
                # DosyayÄ± kaydet
                os.makedirs(os.path.dirname(output_path), exist_ok=True)
                
                total_size = int(response.headers.get('content-length', 0))
                downloaded_size = 0
                
                with open(output_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        if chunk:
                            f.write(chunk)
                            downloaded_size += len(chunk)
                            
                            # Ä°lerleme gÃ¶ster
                            if total_size > 0:
                                progress = (downloaded_size / total_size) * 100
                                if downloaded_size % (1024 * 1024) == 0:  # Her MB'da log
                                    self.log(f"ğŸ“¥ Ä°ndiriliyor: {progress:.1f}% ({downloaded_size // (1024*1024)}MB/{total_size // (1024*1024)}MB)")
                
                file_size = os.path.getsize(output_path)
                
                # Dosya boyutu kontrolÃ¼
                if file_size < 10240:  # 10KB'den kÃ¼Ã§Ã¼kse
                    self.log(f"âš ï¸ Video dosyasÄ± Ã§ok kÃ¼Ã§Ã¼k ({file_size} bytes)")
                    os.remove(output_path)
                    return False
                
                self.log(f"âœ… Video indirildi: {output_path} ({file_size // (1024*1024)}MB)")
                return True
            else:
                self.log(f"âŒ Video indirme hatasÄ±: HTTP {response.status_code}")
                return False
                
        except Exception as e:
            self.log(f"âŒ Video dosyasÄ± indirme hatasÄ±: {str(e)}")
            return False
    
    def download_video_with_direct_http(self, video_url, output_path):
        """DoÄŸrudan HTTP ile video indirir (FFmpeg alternatifi)"""
        try:
            self.log(f"ğŸŒ DoÄŸrudan HTTP ile video indiriliyor: {video_url}")
            
            # Facebook'un mobil versiyonunu dene (daha basit HTML)
            mobile_url = video_url.replace('www.facebook.com', 'm.facebook.com')
            
            # Mobil sayfa iÃ§eriÄŸini al
            mobile_headers = {
                'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 14_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.0 Mobile/15E148 Safari/604.1',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'en-US,en;q=0.5',
                'Referer': 'https://m.facebook.com/',
                'Connection': 'keep-alive'
            }
            
            response = self.session.get(mobile_url, headers=mobile_headers, timeout=30)
            
            if response.status_code == 200:
                content = response.text
                
                # Mobil sayfada video URL pattern'leri
                mobile_patterns = [
                    r'"videoUrl":"([^"]+)"',
                    r'"src":"([^"]+\.mp4[^"]*?)"',
                    r'data-sigil="inlineVideo"[^>]*src="([^"]+)"',
                    r'<video[^>]*src="([^"]+)"',
                    r'"playable_url":"([^"]+)"'
                ]
                
                for pattern in mobile_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        decoded_url = match.replace('\\/', '/').replace('\\u0026', '&')
                        if decoded_url.startswith('http') and '.mp4' in decoded_url:
                            self.log(f"âœ… Mobil sayfadan video URL bulundu: {decoded_url[:100]}...")
                            return self._download_video_file(decoded_url, output_path)
            
            # Mobil baÅŸarÄ±sÄ±z olursa, embed URL'i dene
            return self._try_embed_video_download(video_url, output_path)
            
        except Exception as e:
            self.log(f"âŒ DoÄŸrudan HTTP video indirme hatasÄ±: {str(e)}")
            return False
    
    def _try_embed_video_download(self, video_url, output_path):
        """Embed video URL'i ile indirme dener"""
        try:
            self.log("ğŸ”— Embed video URL'i deneniyor...")
            
            # Video ID'sini Ã§Ä±kar
            video_id = self._extract_video_id(video_url)
            if not video_id:
                return False
            
            # Embed URL'i oluÅŸtur
            embed_url = f"https://www.facebook.com/plugins/video.php?href={video_url}"
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Referer': 'https://www.facebook.com/'
            }
            
            response = self.session.get(embed_url, headers=headers, timeout=30)
            
            if response.status_code == 200:
                content = response.text
                
                # Embed sayfasÄ±nda video URL pattern'leri
                embed_patterns = [
                    r'"videoUrl":"([^"]+)"',
                    r'"src":"([^"]+\.mp4[^"]*?)"',
                    r'"playable_url":"([^"]+)"',
                    r'hd_src:"([^"]+)"',
                    r'sd_src:"([^"]+)"'
                ]
                
                for pattern in embed_patterns:
                    matches = re.findall(pattern, content)
                    for match in matches:
                        decoded_url = match.replace('\\/', '/').replace('\\u0026', '&')
                        if decoded_url.startswith('http') and '.mp4' in decoded_url:
                            self.log(f"âœ… Embed sayfasÄ±ndan video URL bulundu: {decoded_url[:100]}...")
                            return self._download_video_file(decoded_url, output_path)
            
            return False
            
        except Exception as e:
            self.log(f"âŒ Embed video indirme hatasÄ±: {str(e)}")
            return False
    
    def download_video_alternative(self, video_url, output_path):
        """HTTP/API tabanlÄ± video indirme yÃ¶ntemleri"""
        self.log(f"ğŸŒ HTTP/API tabanlÄ± yÃ¶ntemlerle video indiriliyor: {video_url}")
        
        # YÃ¶ntem 1: Facebook Graph API
        self.log("ğŸ”§ YÃ¶ntem 1: Facebook Graph API deneniyor...")
        if self.download_video_with_http_api(video_url, output_path):
            return True
        
        # YÃ¶ntem 2: DoÄŸrudan HTTP scraping
        self.log("ğŸ”§ YÃ¶ntem 2: HTTP scraping deneniyor...")
        if self._download_video_with_http_scraping(video_url, output_path):
            return True
        
        # YÃ¶ntem 3: Mobil ve embed URL'leri
        self.log("ğŸ”§ YÃ¶ntem 3: Mobil/embed HTTP deneniyor...")
        if self.download_video_with_direct_http(video_url, output_path):
            return True
        
        # YÃ¶ntem 4: DoÄŸrudan regex ile video URL Ã§Ä±karma
        self.log("ğŸ”§ YÃ¶ntem 4: DoÄŸrudan video URL Ã§Ä±karma deneniyor...")
        if self._download_video_direct(video_url, output_path):
            return True
        
        self.log("âŒ TÃ¼m HTTP/API tabanlÄ± video indirme yÃ¶ntemleri baÅŸarÄ±sÄ±z oldu")
        return False
    
    def _download_video_direct(self, video_url, output_path):
        """DoÄŸrudan video URL'i Ã§Ä±kararak indirir"""
        try:
            self.log("ğŸ” Sayfa iÃ§eriÄŸinden video URL'i aranÄ±yor...")
            
            content = self.get_page_content(video_url)
            if not content:
                return False
            
            # Video URL pattern'leri
            video_patterns = [
                r'"playable_url":"([^"]+)"',
                r'"playable_url_quality_hd":"([^"]+)"',
                r'"video_url":"([^"]+)"',
                r'"src":"([^"]+\.mp4[^"]*)"',
                r'"url":"([^"]+\.mp4[^"]*)"',
                r'hd_src:"([^"]+)"',
                r'sd_src:"([^"]+)"',
                r'"representation_url":"([^"]+)"'
            ]
            
            video_urls = []
            
            for pattern in video_patterns:
                matches = re.findall(pattern, content)
                for match in matches:
                    # URL'yi decode et
                    decoded_url = match.replace('\\/', '/').replace('\\u0026', '&').replace('\\u003d', '=')
                    if decoded_url.startswith('http') and any(ext in decoded_url for ext in ['.mp4', '.webm', '.avi']):
                        video_urls.append(decoded_url)
            
            if not video_urls:
                self.log("âŒ Sayfa iÃ§eriÄŸinde video URL'i bulunamadÄ±")
                return False
            
            # En iyi kaliteli URL'i seÃ§ (genellikle en uzun olan)
            best_url = max(video_urls, key=len)
            self.log(f"âœ… Video URL bulundu: {best_url[:100]}...")
            
            # Video dosyasÄ±nÄ± indir
            return self._download_video_file(best_url, output_path)
            
        except Exception as e:
            self.log(f"âŒ DoÄŸrudan video indirme hatasÄ±: {str(e)}")
            return False

    def close(self):
        """Scraper'Ä± kapatÄ±r ve cookie'leri kaydeder"""
        self.save_cookies()
        self.session.close()
        self.log("ğŸ”’ Facebook Request Scraper kapatÄ±ldÄ±")